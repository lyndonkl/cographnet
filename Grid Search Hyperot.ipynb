{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad1e6afa-43c2-4f1a-bbc8-432a18246783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:                                                                                                                                                                                                   \n",
      "{'batch_size': 496.0, 'dropout_co_graph_enabled': False, 'dropout_final_enabled': False, 'dropout_fusion_enabled': False, 'dropout_rate_co_graph': 0.4643111397148273, 'dropout_rate_final': 0.34970308072328576, 'dropout_rate_fusion': 0.45256714273560494, 'dropout_rate_sent': 0.42843059544507023, 'dropout_rate_word': 0.1774289678106419, 'dropout_sent_enabled': False, 'dropout_word_enabled': False, 'gamma': 4.000674057390592, 'hidden_dim': 79.0, 'learning_rate': 2.0791434687645655e-05, 'num_sentence_layers': 4.0, 'num_word_layers': 5.0, 'weight_decay': 1.6769135932290969e-06}\n",
      "Loading existing valid indices from metadata                                                                                                                                                                       \n",
      "Found 10409 processed documents                                                                                                                                                                                    \n",
      "  0%|                                                                                                                                                                       | 0/40 [00:05<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: CoGraphNet.__init__() got an unexpected keyword argument 'num_sentence_layers'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                       | 0/40 [00:06<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CoGraphNet.__init__() got an unexpected keyword argument 'num_sentence_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 223\u001b[39m\n\u001b[32m    198\u001b[39m space = {\n\u001b[32m    199\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m: hp.loguniform(\u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m, np.log(\u001b[32m1e-5\u001b[39m), np.log(\u001b[32m1e-3\u001b[39m)),\n\u001b[32m    200\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mgamma\u001b[39m\u001b[33m'\u001b[39m: hp.uniform(\u001b[33m'\u001b[39m\u001b[33mgamma\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m5.0\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdropout_rate_final\u001b[39m\u001b[33m'\u001b[39m: hp.uniform(\u001b[33m'\u001b[39m\u001b[33mdropout_rate_final\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m0.5\u001b[39m)\n\u001b[32m    220\u001b[39m }\n\u001b[32m    222\u001b[39m trials = Trials()\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m best = \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtpe\u001b[49m\u001b[43m.\u001b[49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest hyperparameters:\u001b[39m\u001b[33m\"\u001b[39m, best)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/cographnet/lib/python3.12/site-packages/hyperopt/fmin.py:540\u001b[39m, in \u001b[36mfmin\u001b[39m\u001b[34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[39m\n\u001b[32m    537\u001b[39m     fn = __objective_fmin_wrapper(fn)\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[33m\"\u001b[39m\u001b[33mfmin\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m=\u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(trials_save_file):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/cographnet/lib/python3.12/site-packages/hyperopt/base.py:671\u001b[39m, in \u001b[36mTrials.fmin\u001b[39m\u001b[34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[32m    667\u001b[39m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[32m    668\u001b[39m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[32m    669\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfmin\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m=\u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/cographnet/lib/python3.12/site-packages/hyperopt/fmin.py:586\u001b[39m, in \u001b[36mfmin\u001b[39m\u001b[34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[39m\n\u001b[32m    583\u001b[39m rval.catch_eval_exceptions = catch_eval_exceptions\n\u001b[32m    585\u001b[39m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m586\u001b[39m \u001b[43mrval\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[32m    589\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials.trials) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/cographnet/lib/python3.12/site-packages/hyperopt/fmin.py:364\u001b[39m, in \u001b[36mFMinIter.exhaust\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    363\u001b[39m     n_done = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.trials)\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m     \u001b[38;5;28mself\u001b[39m.trials.refresh()\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/cographnet/lib/python3.12/site-packages/hyperopt/fmin.py:300\u001b[39m, in \u001b[36mFMinIter.run\u001b[39m\u001b[34m(self, N, block_until_done)\u001b[39m\n\u001b[32m    297\u001b[39m     time.sleep(\u001b[38;5;28mself\u001b[39m.poll_interval_secs)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    299\u001b[39m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28mself\u001b[39m.trials.refresh()\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trials_save_file != \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/cographnet/lib/python3.12/site-packages/hyperopt/fmin.py:178\u001b[39m, in \u001b[36mFMinIter.serial_evaluate\u001b[39m\u001b[34m(self, N)\u001b[39m\n\u001b[32m    176\u001b[39m ctrl = base.Ctrl(\u001b[38;5;28mself\u001b[39m.trials, current_trial=trial)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    180\u001b[39m     logger.error(\u001b[33m\"\u001b[39m\u001b[33mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % \u001b[38;5;28mstr\u001b[39m(e))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/cographnet/lib/python3.12/site-packages/hyperopt/base.py:892\u001b[39m, in \u001b[36mDomain.evaluate\u001b[39m\u001b[34m(self, config, ctrl, attach_attachments)\u001b[39m\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    884\u001b[39m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[32m    885\u001b[39m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[32m    886\u001b[39m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[32m    887\u001b[39m     pyll_rval = pyll.rec_eval(\n\u001b[32m    888\u001b[39m         \u001b[38;5;28mself\u001b[39m.expr,\n\u001b[32m    889\u001b[39m         memo=memo,\n\u001b[32m    890\u001b[39m         print_node_on_error=\u001b[38;5;28mself\u001b[39m.rec_eval_print_node_on_error,\n\u001b[32m    891\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m892\u001b[39m     rval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np.number)):\n\u001b[32m    895\u001b[39m     dict_rval = {\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m: STATUS_OK}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m     91\u001b[39m dropout_rate = {\n\u001b[32m     92\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mword\u001b[39m\u001b[33m'\u001b[39m: params[\u001b[33m'\u001b[39m\u001b[33mdropout_rate_word\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     93\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msent\u001b[39m\u001b[33m'\u001b[39m: params[\u001b[33m'\u001b[39m\u001b[33mdropout_rate_sent\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     96\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfinal\u001b[39m\u001b[33m'\u001b[39m: params[\u001b[33m'\u001b[39m\u001b[33mdropout_rate_final\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     97\u001b[39m }\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Build the model using the hyperparameters\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m model = \u001b[43mCoGraphNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mword_in_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43msent_in_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhidden_dim\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_word_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_word_layers\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_sentence_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_sentence_layers\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout_rate\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Setup optimizer and criterion\u001b[39;00m\n\u001b[32m    112\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=params[\u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m], weight_decay=params[\u001b[33m'\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mTypeError\u001b[39m: CoGraphNet.__init__() got an unexpected keyword argument 'num_sentence_layers'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from torch_geometric.loader import DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "# Import your existing modules\n",
    "from src.models import CoGraphNet\n",
    "from src.data.document_dataset import DocumentGraphDataset\n",
    "from src.train import FocalLoss\n",
    "\n",
    "# -------------------------\n",
    "# Utility Functions\n",
    "# -------------------------\n",
    "def get_all_categories(train_dir: str, test_dir: str):\n",
    "    \"\"\"Get all unique categories across all datasets.\"\"\"\n",
    "    categories = set()\n",
    "    for data_dir in [train_dir, test_dir]:\n",
    "        for file in Path(data_dir).glob('*.json'):\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                doc = json.load(f)\n",
    "                if 'text' in doc and 'category' in doc and doc['text'].strip() and doc['category'].strip():\n",
    "                    categories.add(doc['category'])\n",
    "    return categories\n",
    "\n",
    "def create_dataloaders(root: str, train_dir: str, test_dir: str, batch_size: int):\n",
    "    \"\"\"Create DataLoader instances with a train/validation split.\"\"\"\n",
    "    all_categories = get_all_categories(train_dir, test_dir)\n",
    "    category_to_idx = {cat: idx for idx, cat in enumerate(sorted(all_categories))}\n",
    "    num_classes = len(category_to_idx)\n",
    "    \n",
    "    full_train_dataset = DocumentGraphDataset(\n",
    "        f\"{root}/train\", \n",
    "        train_dir, \n",
    "        category_to_idx=category_to_idx\n",
    "    )\n",
    "    \n",
    "    # Use 80% for training and 20% for validation.\n",
    "    dataset_size = len(full_train_dataset)\n",
    "    val_size = int(dataset_size * 0.2)\n",
    "    train_size = dataset_size - val_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(full_train_dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, num_classes, category_to_idx\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"\n",
    "    Hyperopt objective function.\n",
    "    Trains the model for a few epochs using the provided hyperparameters and returns the validation loss.\n",
    "    \"\"\"\n",
    "    print(\"Hyperparameters:\", params)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Convert batch_size to integer\n",
    "    batch_size = int(params['batch_size'])\n",
    "    root = \"processed_graphs_ohsumed\"\n",
    "    train_dir = \"processed_data_ohsumed/train\"\n",
    "    test_dir = \"processed_data_ohsumed/test\"\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader, num_classes, category_to_idx = create_dataloaders(root, train_dir, test_dir, batch_size)\n",
    "    \n",
    "    # Create dropout configuration from hyperparameters\n",
    "    dropout_config = {\n",
    "        'word': params['dropout_word_enabled'],\n",
    "        'sent': params['dropout_sent_enabled'],\n",
    "        'fusion': params['dropout_fusion_enabled'],\n",
    "        'co_graph': params['dropout_co_graph_enabled'],\n",
    "        'final': params['dropout_final_enabled']\n",
    "    }\n",
    "    \n",
    "    dropout_rate = {\n",
    "        'word': params['dropout_rate_word'],\n",
    "        'sent': params['dropout_rate_sent'],\n",
    "        'fusion': params['dropout_rate_fusion'],\n",
    "        'co_graph': params['dropout_rate_co_graph'],\n",
    "        'final': params['dropout_rate_final']\n",
    "    }\n",
    "    \n",
    "    # Build the model using the hyperparameters\n",
    "    model = CoGraphNet(\n",
    "        word_in_channels=768,\n",
    "        sent_in_channels=768,\n",
    "        hidden_channels=int(params['hidden_dim']),\n",
    "        num_word_layers=int(params['num_word_layers']),\n",
    "        num_sent_layers=int(params['num_sentence_layers']),\n",
    "        num_classes=num_classes,\n",
    "        dropout_config=dropout_config,\n",
    "        dropout_rate=dropout_rate\n",
    "    ).to(device)\n",
    "    \n",
    "    # Setup optimizer and criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "    \n",
    "    # Compute separate class weights for training and validation sets\n",
    "    train_labels = torch.tensor([data.y.item() for data in train_loader.dataset])\n",
    "    train_class_counts = torch.bincount(train_labels, minlength=num_classes)\n",
    "    train_total_samples = len(train_labels)\n",
    "    train_class_weights = train_total_samples / (num_classes * train_class_counts.float())\n",
    "    train_class_weights = train_class_weights.to(device)\n",
    "\n",
    "    val_labels = torch.tensor([data.y.item() for data in val_loader.dataset])\n",
    "    val_class_counts = torch.bincount(val_labels, minlength=num_classes)\n",
    "    val_total_samples = len(val_labels)\n",
    "    val_class_weights = val_total_samples / (num_classes * val_class_counts.float())\n",
    "    val_class_weights = val_class_weights.to(device)\n",
    "    \n",
    "    # Create separate loss functions for training and validation\n",
    "    train_criterion = FocalLoss(gamma=params['gamma'], weight=train_class_weights)\n",
    "    val_criterion = FocalLoss(gamma=params['gamma'], weight=val_class_weights)\n",
    "    \n",
    "    num_epochs = 5  # Use a small number of epochs for hyperparameter tuning\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False, ncols=100):\n",
    "            # Move data to device\n",
    "            word_x = batch['word'].x.to(device)\n",
    "            word_edge_index = batch['word', 'co_occurs', 'word'].edge_index.to(device)\n",
    "            word_edge_weight = batch['word', 'co_occurs', 'word'].edge_attr.to(device)\n",
    "            word_batch = batch['word'].batch.to(device)\n",
    "            \n",
    "            sent_x = batch['sentence'].x.to(device)\n",
    "            sent_edge_index = batch['sentence', 'related_to', 'sentence'].edge_index.to(device)\n",
    "            sent_edge_weight = batch['sentence', 'related_to', 'sentence'].edge_attr.to(device)\n",
    "            sent_batch = batch['sentence'].batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                word_x, word_edge_index, word_batch, word_edge_weight,\n",
    "                sent_x, sent_edge_index, sent_batch, sent_edge_weight\n",
    "            )\n",
    "            batch = batch.to(device)\n",
    "            curr_batch_size = batch.y.size(0)\n",
    "            loss = train_criterion(outputs[:curr_batch_size], batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} Training Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            word_x = batch['word'].x.to(device)\n",
    "            word_edge_index = batch['word', 'co_occurs', 'word'].edge_index.to(device)\n",
    "            word_edge_weight = batch['word', 'co_occurs', 'word'].edge_attr.to(device)\n",
    "            word_batch = batch['word'].batch.to(device)\n",
    "            \n",
    "            sent_x = batch['sentence'].x.to(device)\n",
    "            sent_edge_index = batch['sentence', 'related_to', 'sentence'].edge_index.to(device)\n",
    "            sent_edge_weight = batch['sentence', 'related_to', 'sentence'].edge_attr.to(device)\n",
    "            sent_batch = batch['sentence'].batch.to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                word_x, word_edge_index, word_batch, word_edge_weight,\n",
    "                sent_x, sent_edge_index, sent_batch, sent_edge_weight\n",
    "            )\n",
    "            batch = batch.to(device)\n",
    "            curr_batch_size = batch.y.size(0)\n",
    "            loss = val_criterion(outputs[:curr_batch_size], batch.y)\n",
    "            total_loss += loss.item() * curr_batch_size\n",
    "            preds = outputs[:curr_batch_size].argmax(dim=1)\n",
    "            correct += (preds == batch.y).sum().item()\n",
    "            total += curr_batch_size\n",
    "    \n",
    "    val_loss = total_loss / total\n",
    "    val_acc = correct / total\n",
    "    print(\"Validation Loss:\", val_loss, \"Validation Accuracy:\", val_acc)\n",
    "    \n",
    "    # Hyperopt minimizes the objective, so we return the validation loss.\n",
    "    return {'loss': val_loss, 'status': STATUS_OK}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the hyperopt search space including batch_size and dropout_rate\n",
    "    space = {\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(1e-5), np.log(1e-3)),\n",
    "        'gamma': hp.uniform('gamma', 0.5, 5.0),\n",
    "        'weight_decay': hp.loguniform('weight_decay', np.log(1e-8), np.log(1e-3)),\n",
    "        'hidden_dim': hp.quniform('hidden_dim', 64, 256, 1),\n",
    "        'num_word_layers': hp.quniform('num_word_layers', 1, 5, 1),\n",
    "        'num_sentence_layers': hp.quniform('num_sentence_layers', 1, 5, 1),\n",
    "        'batch_size': hp.quniform('batch_size', 16, 512, 16),\n",
    "        \n",
    "        # Dropout enable/disable flags\n",
    "        'dropout_word_enabled': hp.choice('dropout_word_enabled', [True, False]),\n",
    "        'dropout_sent_enabled': hp.choice('dropout_sent_enabled', [True, False]),\n",
    "        'dropout_fusion_enabled': hp.choice('dropout_fusion_enabled', [True, False]),\n",
    "        'dropout_co_graph_enabled': hp.choice('dropout_co_graph_enabled', [True, False]),\n",
    "        'dropout_final_enabled': hp.choice('dropout_final_enabled', [True, False]),\n",
    "        \n",
    "        # Dropout rates for each component\n",
    "        'dropout_rate_word': hp.uniform('dropout_rate_word', 0.1, 0.5),\n",
    "        'dropout_rate_sent': hp.uniform('dropout_rate_sent', 0.1, 0.5),\n",
    "        'dropout_rate_fusion': hp.uniform('dropout_rate_fusion', 0.1, 0.5),\n",
    "        'dropout_rate_co_graph': hp.uniform('dropout_rate_co_graph', 0.1, 0.5),\n",
    "        'dropout_rate_final': hp.uniform('dropout_rate_final', 0.1, 0.5)\n",
    "    }\n",
    "    \n",
    "    trials = Trials()\n",
    "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=40, trials=trials)\n",
    "    print(\"Best hyperparameters:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d310757-6a18-433d-8c21-ed25cb7dd03b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
